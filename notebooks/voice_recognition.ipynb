{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker Recognition with SpeechBrain ECAPA-TDNN\n",
    "This notebook demonstrates..."
   ],
   "id": "3966e825ad5d1f6c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ],
   "id": "dbba67d31150b8bb"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T17:03:22.947372Z",
     "start_time": "2025-11-16T17:03:03.888146Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "import librosa\n",
    "import torchaudio\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ],
   "id": "be5e415e68beef4c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix torchaudio backend compatibility"
   ],
   "id": "bcdf516551584b9d"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T17:03:55.459641Z",
     "start_time": "2025-11-16T17:03:55.434689Z"
    }
   },
   "outputs": [],
   "source": [
    "if hasattr(torchaudio, 'list_audio_backends'):\n",
    "    backends = torchaudio.list_audio_backends()\n",
    "else:\n",
    "    backends = ['torchcodec']\n",
    "\n",
    "sys.modules['torchaudio'].list_audio_backends = lambda: backends"
   ],
   "id": "fa63f0f25157e064"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monkey-patch huggingface_hub for compatibility"
   ],
   "id": "4e544b1003ce393e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "original_hf_hub_download = huggingface_hub.hf_hub_download\n",
    "\n",
    "def patched_hf_hub_download(*args, **kwargs):\n",
    "    if 'use_auth_token' in kwargs:\n",
    "        kwargs['token'] = kwargs.pop('use_auth_token')\n",
    "    return original_hf_hub_download(*args, **kwargs)\n",
    "\n",
    "huggingface_hub.hf_hub_download = patched_hf_hub_download"
   ],
   "id": "993b62caacc276af"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Pre-trained ECAPA-TDNN Model"
   ],
   "id": "f148706b1ba7ddd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.inference import EncoderClassifier\n",
    "\n",
    "print(\"Loading speechbrain ECAPA-TDNN model...\")\n",
    "\n",
    "classifier = EncoderClassifier.from_hparams(\n",
    "    source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "    savedir=\"pretrained_models/\"\n",
    ")\n",
    "\n",
    "print(\"✓ Model loaded successfully!\")"
   ],
   "id": "1d02b7fc9449154d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Audio Dataset"
   ],
   "id": "3530fd5c56296b36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"audio_features.csv\"\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "print(f\"Loaded {len(df)} audio samples\")\n",
    "print(df.head())\n",
    "\n",
    "speaker_counts = df['member_name'].value_counts()\n",
    "print(speaker_counts)"
   ],
   "id": "736843205525231d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Speaker Distribution"
   ],
   "id": "9da8587ac60dc190"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "speaker_counts.plot(kind='bar', color='steelblue', edgecolor='black')\n",
    "plt.title('Number of Audio Samples per Speaker', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Speaker')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "1d40fad3523b318d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Speaker Embeddings"
   ],
   "id": "dcff498582779b0c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(audio_path):\n",
    "    y, sr = librosa.load(audio_path, sr=16000)\n",
    "    waveform = torch.tensor(y).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        emb = classifier.encode_batch(waveform)\n",
    "    return emb.squeeze().cpu().numpy()"
   ],
   "id": "2943c2fa7610457d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "labels = []\n",
    "failed_files = []\n",
    "\n",
    "print(\"Extracting embeddings...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if idx % 10 == 0:\n",
    "        print(f\"Progress: {idx}/{len(df)} ({idx/len(df)*100:.1f}%)\")\n",
    "\n",
    "    try:\n",
    "        emb = get_embedding(row['audio_path'])\n",
    "        embeddings.append(emb)\n",
    "        labels.append(row['member_name'])\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Skipping {row['audio_path']}: {e}\")\n",
    "        failed_files.append(row['audio_path'])\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"✓ Extracted {len(embeddings)} embeddings\")\n",
    "if failed_files:\n",
    "    print(f\"✗ Failed files: {len(failed_files)}\")\n",
    "\n",
    "if len(embeddings) == 0:\n",
    "    raise ValueError(\"No embeddings extracted!\")"
   ],
   "id": "7333ace38734dcf9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Statistics"
   ],
   "id": "6eaf15ceceec94ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(embeddings)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(X.shape, len(np.unique(y)))\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1,2,1); plt.hist(X.mean(axis=1), bins=30); plt.title(\"Mean Embedding Values\")\n",
    "plt.subplot(1,2,2); plt.hist(X.std(axis=1), bins=30); plt.title(\"Embedding Std Dev\")\n",
    "plt.show()"
   ],
   "id": "212c59416bae5794"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Training and Validation Sets"
   ],
   "id": "6ec3bbfdc651c478"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_enc, test_size=0.3, stratify=y_enc, random_state=42\n",
    ")\n",
    "\n",
    "print(len(X_train), len(X_val))"
   ],
   "id": "e953c9c1a56d1e13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Logistic Regression Classifier"
   ],
   "id": "b552f0a7e3118ff4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Training complete.\")"
   ],
   "id": "d08d452132824dae"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Model Performance"
   ],
   "id": "27405a88683e4365"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_val = model.predict(X_val)\n",
    "\n",
    "print(\"Train Acc:\", accuracy_score(y_train, y_pred_train))\n",
    "print(\"Val Acc:\", accuracy_score(y_val, y_pred_val))\n",
    "print(classification_report(y_val, y_pred_val, target_names=le.classes_))"
   ],
   "id": "f0f3ce1d87d61abb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ],
   "id": "9ae801af3f266f9e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_val, y_pred_val)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.show()"
   ],
   "id": "5c5608b57c3875bd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-Class Metrics"
   ],
   "id": "940ec3f19d8c73f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_val, y_pred_val)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Speaker\": le.classes_,\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1\": f1\n",
    "})\n",
    "metrics_df"
   ],
   "id": "55bd5b1023590931"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ],
   "id": "a653c99b36f7246e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, \"speechbrain_classifier.pkl\")\n",
    "joblib.dump(le, \"speechbrain_label_encoder.pkl\")\n",
    "print(\"Models saved.\")"
   ],
   "id": "c42660bed689b04e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
